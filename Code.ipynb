{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shapefile\n",
    "import pandas\n",
    "from sklearn import datasets, linear_model\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn\n",
    "%matplotlib inline \n",
    "import random\n",
    "import multiprocessing\n",
    "import scipy\n",
    "import copy\n",
    "from scipy.stats import truncnorm\n",
    "\n",
    "\n",
    "import warnings\n",
    "\n",
    "MAX = float('inf')\n",
    "MIN = float('-inf')\n",
    "\n",
    "\n",
    "def get_truncated_normal(mean=0, sd=1, low=0, upp=10):#Taken from https://stackoverflow.com/a/44308018\n",
    "    return truncnorm((low - mean) / sd, (upp - mean) / sd, loc=mean, scale=sd)\n",
    "\n",
    "def asymmetricallyUncertainChange(initial, uncertainty_below, uncertainty_above,\n",
    "                                  minimum=MIN, maximum=MAX):\n",
    "    if initial==0:\n",
    "        return 0.0\n",
    "    lower_bound = max(minimum, initial * (1-uncertainty_below))\n",
    "    upper_bound = min(maximum, initial * (1+uncertainty_above))\n",
    "    standard_deviation = (upper_bound - lower_bound)/6\n",
    "    standard_deviation = max(standard_deviation, -1*standard_deviation)\n",
    "    #print(\"{} , {} , {} , {}\".format(initial, lower_bound, upper_bound, standard_deviation))\n",
    "    new_value = get_truncated_normal(initial, standard_deviation, lower_bound, upper_bound).rvs()\n",
    "    return new_value\n",
    "\n",
    "def symmetricallyUncertainChange(initial, uncertainty, minimum=MIN, maximum=MAX):\n",
    "    return asymmetricallyUncertainChange(initial, uncertainty, uncertainty, minimum, maximum)\n",
    "\n",
    "\n",
    "def makeAsymmetricallyUncertainChanges(dataset, key, uncertainty_below, uncertainty_above, minimum=MIN, maximum=MAX):\n",
    "    for i in range(len(dataset)):\n",
    "        dataset.loc[i,key] = asymmetricallyUncertainChange(dataset[key].values[i], uncertainty_below, uncertainty_above, minimum, maximum)\n",
    "\n",
    "def makeSymmetricallyUncertainChanges(dataset, key, uncertainty, minimum=MIN, maximum=MAX):\n",
    "    makeAsymmetricallyUncertainChanges(dataset, key, uncertainty, uncertainty, minimum, maximum)\n",
    "\n",
    "\n",
    "def project_to_year_from_two(year_wanted, dataset, tag_format, first_year, second_year):\n",
    "    years_between_known = second_year - first_year\n",
    "    years_to_wanted = year_wanted - second_year\n",
    "    first_tag = tag_format.format(first_year)\n",
    "    second_tag = tag_format.format(second_year)\n",
    "    initial_projections = (dataset[second_tag] + years_to_wanted*((dataset[second_tag]-dataset[first_tag])/years_between_known))\n",
    "    for i in range(len(initial_projections)):\n",
    "        initial_projections[i] = max(0, initial_projections.values[i])\n",
    "        if years_to_wanted==0:\n",
    "            if initial_projections[i] - dataset[second_tag][i]:\n",
    "                print(initial_projections[i], dataset[second_tag][i])\n",
    "    return initial_projections\n",
    "\n",
    "\n",
    "def getPredictions(dataset, training_predictors, training_outcomes, desired_predictors, model, type_string):\n",
    "    model.fit(training_predictors, training_outcomes)\n",
    "    training_predictions = model.predict(training_predictors)\n",
    "    \n",
    "    if print_mae:\n",
    "        if type_string in []:#\"NTL Historic\"]:\n",
    "            for i in range(len(training_predictions)):\n",
    "                diff = np.abs(training_predictions[i] - training_outcomes[i])\n",
    "                print(\"{}: {}, {}\".format(diff, training_predictions[i], training_outcomes[i]))\n",
    "            print(training_predictions)\n",
    "            print(training_outcomes)\n",
    "        training_mae = np.mean(np.abs(training_predictions - training_outcomes))\n",
    "        print('{} Model MAE on Training Data: {}'.format(type_string, training_mae))\n",
    "    \n",
    "    if desired_predictors is None:\n",
    "        return training_predictors\n",
    "    return [training_predictors, model.predict(desired_predictors)]\n",
    "\n",
    "def getNTLBasedPredictions(usa_dta):\n",
    "    training_to_year = min(2016, historic_model_year)\n",
    "    desired_from_year = min(2016, desired_year-1)\n",
    "    training_from_year = training_to_year - (desired_year - desired_from_year)\n",
    "    training_predictors = usa_dta[desired_stat.format(training_from_year)].values.reshape(len(usa_dta), 1)\n",
    "    ntl_outcome = usa_dta[desired_stat.format(training_to_year)].values.reshape(len(usa_dta), 1)\n",
    "    desired_predictors = usa_dta[desired_stat.format(desired_from_year)].values.reshape(len(usa_dta), 1)\n",
    "    return getPredictions(usa_dta, training_predictors, ntl_outcome, desired_predictors, linear_model.LinearRegression(), \"NTL Historic\")\n",
    "\n",
    "def getHistoricPredictions(usa_dta):\n",
    "    training_predictors = usa_dta[historical_reshape_on].values.reshape(len(usa_dta), len(historical_reshape_on))\n",
    "    desired_predictors = usa_dta[future_reshape_on].values.reshape(len(usa_dta), len(future_reshape_on))\n",
    "    outcome = usa_dta[desired_stat.format(historic_model_year)].values.reshape(len(usa_dta), 1)\n",
    "    return getPredictions(usa_dta, training_predictors, outcome, desired_predictors, linear_model.LinearRegression(), \"Historic\")\n",
    "\n",
    "def getCombinedHistoricPredictions(usa_dta):\n",
    "    results = getHistoricPredictions(usa_dta)\n",
    "    usa_dta['training_historic_predictions'] = results[0][:,0]\n",
    "    usa_dta['desired_historic_predictions'] = results[1]\n",
    "    results = getNTLBasedPredictions(usa_dta)\n",
    "    usa_dta['training_ntl_based_predictions'] = results[0][:,0]\n",
    "    usa_dta['desired_ntl_based_predictions'] = results[1]\n",
    "    training_predictors = usa_dta[['random', 'training_ntl_based_predictions']].values.reshape(len(usa_dta), 2)\n",
    "    #training_predictors = usa_dta[['training_historic_predictions', 'training_ntl_based_predictions']].values.reshape(len(usa_dta), 2)\n",
    "    training_outcomes = usa_dta[desired_stat.format(historic_model_year)].values.reshape(len(usa_dta), 1)\n",
    "    desired_predictors = usa_dta[['desired_historic_predictions', 'desired_ntl_based_predictions']].values.reshape(len(usa_dta), 2)\n",
    "    \n",
    "    return getPredictions(usa_dta, training_predictors, training_outcomes, desired_predictors, linear_model.ElasticNet(), \"Combined Historic\")[1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Worker 0 starting.\n",
      "Historic Model MAE on Training Data: 2.4877066321128796\n",
      "NTL Historic Model MAE on Training Data: 0.575515324353808\n",
      "Combined Historic Model MAE on Training Data: 5.07006272352097\n",
      "Future Model MAE: 0.8406634856279064\n",
      "Worker 0 finished.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#have NTL data for 2012-2016\n",
    "desired_year = 2016\n",
    "historic_model_year = min(desired_year-1, 2016)\n",
    "desired_stat = 'viirs_vcmcfg_dnb_composites_v10_yearly_max.{}.mean'\n",
    "#we can use the projected pop instead of making our own\n",
    "\n",
    "assert 2013 <= historic_model_year < desired_year <= 2020\n",
    "\n",
    "usa_dta_initial = pandas.read_csv(\"./FinalData.csv\")\n",
    "\n",
    "usa_dta_initial.dropna(inplace=True, axis=1, thresh=len(usa_dta_initial)-10)\n",
    "usa_dta_initial.reset_index(drop=True, inplace=True)\n",
    "usa_dta_initial.dropna(inplace=True, axis=0)\n",
    "usa_dta_initial.reset_index(drop=True, inplace=True)\n",
    "\n",
    "pertinent_and_preds = copy.deepcopy(usa_dta_initial)[['gpw_v4_count.2015.sum',\n",
    "                                                        'gpw_v4_density.2015.mean',\n",
    "                                                        'access_50k.none.mean',\n",
    "                                                        'distance_to_coast_236.none.mean',\n",
    "                                                        'srtm_elevation_500m.none.mean',\n",
    "                                                        'viirs_vcmcfg_dnb_composites_v10_yearly_max.2012.mean',\n",
    "                                                        'viirs_vcmcfg_dnb_composites_v10_yearly_max.2013.mean',\n",
    "                                                        'viirs_vcmcfg_dnb_composites_v10_yearly_max.2014.mean',\n",
    "                                                        'viirs_vcmcfg_dnb_composites_v10_yearly_max.2015.mean',\n",
    "                                                        'viirs_vcmcfg_dnb_composites_v10_yearly_max.2016.mean',\n",
    "                                                        'NAME_2']]\n",
    "\n",
    "use_for_projections = copy.deepcopy(usa_dta_initial)[['gpw_v4_count.2010.sum',\n",
    "                                                         'gpw_v4_count.2015.sum',\n",
    "                                                         'gpw_v4_count.2020.sum',\n",
    "                                                         'gpw_v4_density.2010.mean',\n",
    "                                                         'gpw_v4_density.2015.mean',\n",
    "                                                         'gpw_v4_density.2020.mean',\n",
    "                                                         'ambient_air_pollution_2013_fus_calibrated.2010.mean',\n",
    "                                                         'ambient_air_pollution_2013_fus_calibrated.2011.mean',\n",
    "                                                         'ambient_air_pollution_2013_fus_calibrated.2012.mean',\n",
    "                                                         'ambient_air_pollution_2013_fus_calibrated.2013.mean',\n",
    "                                                        'viirs_vcmcfg_dnb_composites_v10_yearly_max.2012.mean',\n",
    "                                                        'viirs_vcmcfg_dnb_composites_v10_yearly_max.2013.mean',\n",
    "                                                        'viirs_vcmcfg_dnb_composites_v10_yearly_max.2014.mean',\n",
    "                                                        'viirs_vcmcfg_dnb_composites_v10_yearly_max.2015.mean',\n",
    "                                                        'viirs_vcmcfg_dnb_composites_v10_yearly_max.2016.mean',]]\n",
    "\n",
    "\n",
    "if desired_year == 2020:\n",
    "    pertinent_and_preds['gpw_v4_count.{}.sum.prediction'.format(desired_year)] = use_for_projections['gpw_v4_count.{}.sum'.format(desired_year)]\n",
    "    pertinent_and_preds['gpw_v4_density.{}.mean.prediction'.format(desired_year)] = use_for_projections['gpw_v4_density.{}.mean'.format(desired_year)]\n",
    "else:\n",
    "    pertinent_and_preds['gpw_v4_count.{}.sum.prediction'.format(desired_year)] = project_to_year_from_two(desired_year, use_for_projections, 'gpw_v4_count.{}.sum', 2010, 2015)\n",
    "    pertinent_and_preds['gpw_v4_density.{}.mean.prediction'.format(desired_year)] = project_to_year_from_two(desired_year, use_for_projections, 'gpw_v4_density.{}.mean', 2010, 2015)\n",
    "pertinent_and_preds['gpw_v4_count.{}.sum.prediction'.format(historic_model_year)] = project_to_year_from_two(historic_model_year, use_for_projections, 'gpw_v4_count.{}.sum', 2010, 2015)\n",
    "pertinent_and_preds['gpw_v4_density.{}.mean.prediction'.format(historic_model_year)] = project_to_year_from_two(historic_model_year, use_for_projections, 'gpw_v4_density.{}.mean', 2010, 2015)\n",
    "pertinent_and_preds['ambient_air_pollution_2013_fus_calibrated.{}.mean.prediction'.format(historic_model_year)] = project_to_year_from_two(desired_year, use_for_projections, 'ambient_air_pollution_2013_fus_calibrated.{}.mean', 2010, 2013)\n",
    "pertinent_and_preds['ambient_air_pollution_2013_fus_calibrated.{}.mean.prediction'.format(desired_year)] = project_to_year_from_two(desired_year, use_for_projections, 'ambient_air_pollution_2013_fus_calibrated.{}.mean', 2010, 2013)\n",
    "#Confirm that that's better than shooting from 2012 and 2013.\n",
    "pertinent_and_preds['random'] = pertinent_and_preds['gpw_v4_count.2015.sum'].apply(lambda x: (int(x)*0)+ random.random())\n",
    "\n",
    "pertinent_and_preds['{}.prediction'.format(desired_stat).format(desired_year)] = project_to_year_from_two(desired_year, use_for_projections, desired_stat, 2012, 2016)\n",
    "\n",
    "copy that into below:\n",
    "    '{}.prediction'.format(desired_stat).format(desired_year)\n",
    "\n",
    "historical_reshape_on = ['gpw_v4_count.{}.sum.prediction'.format(historic_model_year),\n",
    "                         'gpw_v4_density.{}.mean.prediction'.format(historic_model_year),\n",
    "                         'ambient_air_pollution_2013_fus_calibrated.{}.mean.prediction'.format(historic_model_year),\n",
    "                         'access_50k.none.mean',\n",
    "                         'distance_to_coast_236.none.mean',\n",
    "                         'srtm_elevation_500m.none.mean']\n",
    "\n",
    "future_reshape_on = ['gpw_v4_count.{}.sum.prediction'.format(desired_year),\n",
    "                     'gpw_v4_density.{}.mean.prediction'.format(desired_year),\n",
    "                     'ambient_air_pollution_2013_fus_calibrated.{}.mean.prediction'.format(desired_year),\n",
    "                     'access_50k.none.mean',\n",
    "                     'distance_to_coast_236.none.mean',\n",
    "                     'srtm_elevation_500m.none.mean']\n",
    "\n",
    "print_mae = True\n",
    "\n",
    "def applyUncertainty(usa_dta):\n",
    "    makeSymmetricallyUncertainChanges(usa_dta, 'gpw_v4_count.2015.sum', 0.1, 0)\n",
    "    makeSymmetricallyUncertainChanges(usa_dta, 'gpw_v4_density.2015.mean', 0.1, 0)\n",
    "    if desired_year == 2020:\n",
    "        makeSymmetricallyUncertainChanges(usa_dta, 'gpw_v4_count.{}.sum.prediction'.format(desired_year), 0.3, 0)\n",
    "        makeSymmetricallyUncertainChanges(usa_dta, 'gpw_v4_density.{}.mean.prediction'.format(desired_year), 0.3, 0)\n",
    "    else:\n",
    "        makeSymmetricallyUncertainChanges(usa_dta, 'gpw_v4_count.{}.sum.prediction'.format(desired_year), 0.3, 0)\n",
    "        makeSymmetricallyUncertainChanges(usa_dta, 'gpw_v4_density.{}.mean.prediction'.format(desired_year), 0.3, 0)\n",
    "    makeSymmetricallyUncertainChanges(usa_dta, 'ambient_air_pollution_2013_fus_calibrated.{}.mean.prediction'.format(historic_model_year), 0.25, 0)\n",
    "    makeSymmetricallyUncertainChanges(usa_dta, 'ambient_air_pollution_2013_fus_calibrated.{}.mean.prediction'.format(desired_year), 0.5, 0)\n",
    "    makeSymmetricallyUncertainChanges(usa_dta, 'access_50k.none.mean', 0.25, 0)\n",
    "    makeSymmetricallyUncertainChanges(usa_dta, 'distance_to_coast_236.none.mean', 0.1, 0)\n",
    "    makeSymmetricallyUncertainChanges(usa_dta, 'srtm_elevation_500m.none.mean', 0.1)\n",
    "    makeSymmetricallyUncertainChanges(usa_dta, '{}.prediction'.format(desired_stat).format(desired_year), 0.2, 0)\n",
    "\n",
    "\n",
    "def make_predictions(worker_id):\n",
    "    print(\"Worker {} starting.\".format(worker_id))\n",
    "    scipy.random.seed()\n",
    "\n",
    "    usa_dta = copy.deepcopy(pertinent_and_preds)\n",
    "    #applyUncertainty(usa_dta)\n",
    "    predictions =  getCombinedHistoricPredictions(usa_dta)\n",
    "    #I'm going to save the top 10 locations, and my overall accuracy at the 20% threshold\n",
    "    #(i.e., number of estimates I get within 20%) based on the professionally predicted 2020 data.\n",
    "    if desired_year <= 2016:\n",
    "        abs_diff_pred = np.abs(predictions - usa_dta[desired_stat.format(desired_year)].values)\n",
    "        if print_mae:\n",
    "            print(\"Future Model MAE: {}\".format(np.mean(abs_diff_pred)))\n",
    "        twentyPercent = np.mean(abs_diff_pred <= usa_dta[desired_stat.format(desired_year)].values * 0.2)\n",
    "    \n",
    "    usa_dta['{}.prediction'.format(desired_stat).format(desired_year)] = predictions\n",
    "    top = usa_dta.nlargest(10, '{}.prediction'.format(desired_stat).format(desired_year))\n",
    "    \n",
    "    with open(\"./sim_res/result_\"+str(random.random())+\".csv\",mode='w') as file:\n",
    "        for i in usa_dta.loc[usa_dta[\"NAME_2\"].isin(top[\"NAME_2\"].values)].NAME_2.values:\n",
    "            file.write(i + \",\")\n",
    "        if desired_year <= 2016:\n",
    "            file.write(str(np.mean(twentyPercent)))\n",
    "    \n",
    "    print(\"Worker {} finished.\".format(worker_id))\n",
    "\n",
    "\n",
    "usable_threads = multiprocessing.cpu_count()*2-2\n",
    "pool = multiprocessing.Pool(usable_threads)\n",
    "\n",
    "iterations = 1\n",
    "result = pool.map(make_predictions, range(iterations))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os   \n",
    "import pandas\n",
    "import collections\n",
    "import seaborn\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "\n",
    "all_files = glob.glob(os.path.join('./sim_res/', \"*.csv\"))   \n",
    "df_set = (pandas.read_csv(f, header=None) for f in all_files)\n",
    "all_simulation_results = pandas.concat(df_set, ignore_index=True)\n",
    "\n",
    "if desired_year <= 2016:\n",
    "    accuracy_metric = all_simulation_results[10].values\n",
    "    all_simulation_results = all_simulation_results.drop(all_simulation_results.columns[10], axis=1)\n",
    "    print(accuracy_metric)\n",
    "\n",
    "flat_list = [item for sublist in all_simulation_results.values for item in sublist]\n",
    "noNAList = [x for x in flat_list if str(x) != 'nan']\n",
    "district_counts = collections.Counter(noNAList)\n",
    "plt.bar(range(len(district_counts)), list(district_counts.values()), align='center')\n",
    "plt.xticks(range(len(district_counts)), list(district_counts.keys()), rotation=\"vertical\")\n",
    "plt.ylabel(\"Number of Iterations in Top 10\")\n",
    "plt.xlabel(\"District Name\")\n",
    "plt.title(\"Districts in Top 10\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for /: 'str' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-45869c1263e5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdesired_year\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m2016\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mseaborn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy_metric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_hist\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkde\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/anaconda3-4.4.0/lib/python3.6/site-packages/seaborn/distributions.py\u001b[0m in \u001b[0;36mdistplot\u001b[0;34m(a, bins, hist, kde, rug, fit, hist_kws, kde_kws, rug_kws, fit_kws, color, vertical, norm_hist, axlabel, label, ax)\u001b[0m\n\u001b[1;32m    190\u001b[0m             \u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m             \u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m         \u001b[0mcolor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_color\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3-4.4.0/lib/python3.6/site-packages/numpy/core/_methods.py\u001b[0m in \u001b[0;36m_mean\u001b[0;34m(a, axis, dtype, out, keepdims)\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mrcount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mret\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mrcount\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for /: 'str' and 'int'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3-4.4.0/lib/python3.6/site-packages/matplotlib/font_manager.py:1297: UserWarning: findfont: Font family ['sans-serif'] not found. Falling back to DejaVu Sans\n",
      "  (prop.get_family(), self.defaultFamily[fontext]))\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAD8CAYAAAB0IB+mAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nGHxJKsLgS1IR/wcLMdegE3dW6gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2ad8009974e0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if desired_year <= 2016:\n",
    "    seaborn.distplot(accuracy_metric, norm_hist=False, kde=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
